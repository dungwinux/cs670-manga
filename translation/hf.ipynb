{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# default: Load the model on the available device(s)\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\", \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=\"/scratch/workspace/ctpham_umass_edu-llama/.cache/\"\n",
    ")\n",
    "\n",
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-stage experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"data/030.jpg\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\", \n",
    "                \"text\": open(\"prompts/text_extraction.md\").read()},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    " \n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=2000, do_sample=False)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse order of extracted texts to fit manga reading order\n",
    "\n",
    "def reverse_texts_with_tag_renaming(detected):\n",
    "    # Pattern to find each row with its content\n",
    "    row_pattern = r'(<row_\\d+>)(.*?)(</row_\\d+>)'\n",
    "    # Pattern to find individual <text_n> elements within each row\n",
    "    text_pattern = r'(<text_\\d+>)(.*?)(</text_\\d+>)'\n",
    "    \n",
    "    # Function to reverse texts in a single row\n",
    "    def reverse_row(match):\n",
    "        row_start, row_content, row_end = match.groups()\n",
    "        # Find all <text_n> elements in the row\n",
    "        texts = re.findall(text_pattern, row_content, re.DOTALL)\n",
    "        # Reverse the order and rename tags to reflect the new order\n",
    "        reversed_texts = []\n",
    "        for i, (_, text_content, _) in enumerate(texts[::-1], start=1):\n",
    "            # Update the tag name to match the new order (e.g., <text_1>, <text_2>, ...)\n",
    "            new_text = f\"<text_{i}>\\n{text_content}\\n</text_{i}>\"\n",
    "            reversed_texts.append(new_text)\n",
    "        # Join reversed and renamed texts\n",
    "        return f\"{row_start}\\n    \" + \"\\n    \".join(reversed_texts) + f\"\\n{row_end}\"\n",
    "    \n",
    "    # Apply the reverse_row function to each row match\n",
    "    reversed_detected = re.sub(row_pattern, reverse_row, detected, flags=re.DOTALL)\n",
    "    \n",
    "    return reversed_detected\n",
    "\n",
    "# Reverse the texts in each row, rename tags, and print the result\n",
    "reversed_output = reverse_texts_with_tag_renaming(output_text[0])\n",
    "print(reversed_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translating texts\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"data/030.jpg\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\", \n",
    "                \"text\": open(\"translation.md\").read().format(orig=reversed_output)},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    " \n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=2000, do_sample=False)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating detected text only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1: Extracting text from comic strips\n",
    "import pandas as pd\n",
    "\n",
    "json_structure = pd.read_csv('data/annotation/annotation_cleaned.csv')\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                #\"image\": \"images/balloon_dream/ja/000.jpg\",\n",
    "                \"image\": \"data/000.jpg\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\", \n",
    "                \"text\": open(\"prompts/translation_json.md\").read()},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    " \n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=2000, do_sample=False)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep for large batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean json file\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Function to extract book data from JSON\n",
    "def extract_book_data(json_data):\n",
    "    books_data = []\n",
    "    for book in json_data:\n",
    "        book_title = book.get(\"book_title\", \"\")\n",
    "        \n",
    "        for page in book.get(\"pages\", []):\n",
    "            image_path = page.get(\"image_paths\", {}).get(\"ja\", \"\")\n",
    "            \n",
    "            # Extract each 'text_ja' entry as a separate row\n",
    "            for text_entry in page.get(\"text\", []):\n",
    "                text_ja = text_entry.get(\"text_ja\", \"\")\n",
    "                text_en = text_entry.get(\"text_en\", \"\")\n",
    "                if text_ja:\n",
    "                    books_data.append({\n",
    "                        \"book_title\": book_title,\n",
    "                        \"image_path\": image_path,\n",
    "                        \"text_ja\": text_ja, \n",
    "                        \"text_en\": text_en\n",
    "                    })\n",
    "    return books_data\n",
    "\n",
    "# Load JSON data from file\n",
    "with open('data/annotation/annotation.json', 'r', encoding='utf-8') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Extract structured data for DataFrame\n",
    "books_data_structure = extract_book_data(json_data)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(books_data_structure, columns=[\"book_title\", \"image_path\", \"text_ja\", \"text_en\"])\n",
    "\n",
    "df['book_index'] = df.groupby('image_path').cumcount()\n",
    "\n",
    "# Update the image_book_concat column to include the book-specific index\n",
    "df[\"image_book_concat\"] = df[\"image_path\"] + \"_\" + df[\"book_index\"].astype(str)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.to_csv(\"data/annotation/annotation_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post processing items \n",
    "import pandas as pd\n",
    "df_new = pd.read_csv(\"data/annotation/qwen.csv\")\n",
    "# Define a function to process each row of 'outputs' and segment it into 'orig', 'translated', and duplicate 'image_path'\n",
    "def segment_outputs(row):\n",
    "    items = row['outputs'].split(\"\\n\\n\")\n",
    "    segmented_data = []\n",
    "    \n",
    "    # Parsing each item for 'orig' and 'translated' texts\n",
    "    for item in items:\n",
    "        orig_text = \"\"\n",
    "        translated_text = \"\"\n",
    "        lines = item.split(\"\\n\")\n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"<text>\"):\n",
    "                orig_text = lines[i+1].strip()\n",
    "            elif line.startswith(\"<translation>\"):\n",
    "                translated_text = lines[i+1].strip()\n",
    "        \n",
    "        segmented_data.append({\n",
    "            'text_ja': orig_text,\n",
    "            'translated': translated_text,\n",
    "            'image_path': row['image_path']\n",
    "        })\n",
    "    return segmented_data\n",
    "\n",
    "# Apply the function to each row in the DataFrame and concatenate results\n",
    "all_segmented_data = [segment for _, row in df_new.iterrows() for segment in segment_outputs(row)]\n",
    "segmented_df = pd.DataFrame(all_segmented_data)\n",
    "df = pd.read_csv(\"data/annotation/annotation_cleaned.csv\")\n",
    "#segmented_df = segmented_df.rename(columns={'orig': 'text_ja'})\n",
    "segmented_df.merge(df, on=[\"image_path\", 'text_ja']).reset_index(drop=True).to_csv(\"data/annotation/qwen_7b_final.csv\", index=False)\n",
    "segmented_df.to_csv(\"data/annotation/qwen_7b_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post processing items \n",
    "import pandas as pd\n",
    "df_new = pd.read_csv(\"data/annotation/qwen_72.csv\")\n",
    "# Define a function to process each row of 'outputs' and segment it into 'orig', 'translated', and duplicate 'image_path'\n",
    "def segment_outputs(row):\n",
    "    items = row['outputs'].split(\"\\n\\n\")\n",
    "    segmented_data = []\n",
    "    \n",
    "    # Parsing each item for 'orig' and 'translated' texts\n",
    "    for item in items:\n",
    "        orig_text = \"\"\n",
    "        translated_text = \"\"\n",
    "        lines = item.split(\"\\n\")\n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"<text>\"):\n",
    "                orig_text = lines[i+1].strip()\n",
    "            elif line.startswith(\"<translation>\"):\n",
    "                translated_text = lines[i+1].strip()\n",
    "        \n",
    "        segmented_data.append({\n",
    "            'text_ja': orig_text,\n",
    "            'translated': translated_text,\n",
    "            'image_path': row['image_path']\n",
    "        })\n",
    "    return segmented_data\n",
    "\n",
    "# Apply the function to each row in the DataFrame and concatenate results\n",
    "all_segmented_data = [segment for _, row in df_new.iterrows() for segment in segment_outputs(row)]\n",
    "segmented_df = pd.DataFrame(all_segmented_data)\n",
    "df = pd.read_csv(\"data/annotation/annotation_cleaned.csv\")\n",
    "#segmented_df = segmented_df.rename(columns={'orig': 'text_ja'})\n",
    "segmented_df.merge(df, on=[\"image_path\", 'text_ja']).reset_index(drop=True).to_csv(\"data/annotation/qwen_72b_final.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sacrebleu\n",
    "from sacrebleu import corpus_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from bert_score import score as bert_score\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "import torch\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "seventytwo = pd.read_csv(\"../data/output/openmantra_translation-final.csv\")\n",
    "seventytwo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with 7b to find text_en \n",
    "seven = pd.read_csv('../data/output/qwen_7b_final.csv')\n",
    "seven.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seventytwo.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging \n",
    "df = pd.merge(seventytwo,seven, left_on=[\"original\", \"path\"], right_on=[\"text_ja\", \"image_path\"], how=\"inner\")[['detection_path', 'path', 'coordinates', 'outputs', 'original',\n",
    "      'translation', 'translated', 'text_en']]\n",
    "df.to_csv(\"../data/output/openmantra_comp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "DEEPL_URL = \"https://api-free.deepl.com/v2/translate\"  # Use 'api.deepl.com' if you have a paid plan.\n",
    "\n",
    "# Function to call DeepL API\n",
    "def translate_text(text):\n",
    "    if not text.strip():  # Skip empty text\n",
    "        return text\n",
    "    params = {\n",
    "        \"auth_key\": DEEPL_API_KEY,\n",
    "        \"text\": text,\n",
    "        \"target_lang\": \"EN\"  # English target language\n",
    "    }\n",
    "    response = requests.post(DEEPL_URL, data=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"translations\"][0][\"text\"]\n",
    "    else:\n",
    "        print(f\"Error translating text: {response.status_code}, {response.text}\")\n",
    "        return text  # Return original text if translation fails\n",
    "\n",
    "# Apply translation to the DataFrame\n",
    "df['deepl'] = df['original'].apply(translate_text)\n",
    "df.to_csv(\"../data/output/openmantra_comp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/output/openmantra_comp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu import corpus_bleu\n",
    "\n",
    "hypotheses = df['deepl'].tolist()  # List of system translations\n",
    "references = [[ref] for ref in df['text_en'].tolist()]  # Wrap each reference in a list for sacrebleu\n",
    "\n",
    "# Calculate SacreBLEU\n",
    "sacrebleu_score = corpus_bleu(hypotheses, references).score\n",
    "print(f\"SacreBLEU Score: {sacrebleu_score}\")\n",
    "\n",
    "# Calculate ROUGE-L\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "rouge_l_scores = [scorer.score(ref, hyp)['rougeL'].fmeasure for ref, hyp in zip(df['text_en'], df['deepl'])]\n",
    "average_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores) if rouge_l_scores else 0.0\n",
    "print(f\"Average ROUGE-L Score: {average_rouge_l}\")\n",
    "\n",
    "# METEOR Score\n",
    "# Tokenize references and hypotheses\n",
    "references = [ref.split() for ref in df['text_en'].tolist()]  # Tokenize each reference translation\n",
    "hypotheses = [hyp.split() for hyp in df['deepl'].tolist()]  # Tokenize each system translation\n",
    "\n",
    "# Calculate METEOR scores\n",
    "meteor_scores = [meteor_score([ref], hyp) for ref, hyp in zip(references, hypotheses)]\n",
    "meteor = sum(meteor_scores) / len(meteor_scores)\n",
    "\n",
    "# TER Score (using edit distance normalized by reference length)\n",
    "def ter_score(hypotheses: List[str], references: List[List[str]]) -> float:\n",
    "    total_edits = 0\n",
    "    total_ref_length = 0\n",
    "    for hyp, refs in zip(hypotheses, references):\n",
    "        ref = refs[0]\n",
    "        total_edits += nltk.edit_distance(hyp, ref)\n",
    "        total_ref_length += len(ref.split())\n",
    "    return total_edits / total_ref_length if total_ref_length > 0 else 0\n",
    "\n",
    "ter = ter_score(hypotheses, references)\n",
    "\n",
    "# ChrF score\n",
    "hypotheses = df['deepl'].tolist()  # Machine-generated translations as a list of full sentences\n",
    "references = [df['text_en'].tolist()]  # Wrap in a list to match sacrebleu's expected format for multiple references\n",
    "\n",
    "# Calculate chrF score\n",
    "chrf = sacrebleu.corpus_chrf(hypotheses, references).score\n",
    "\n",
    "hypotheses = df['deepl'].tolist()  # Machine-generated translations\n",
    "references = df['text_en'].tolist()  # Reference translations\n",
    "\n",
    "# Ensure lengths match\n",
    "assert len(hypotheses) == len(references), \"Mismatch in length between hypotheses and references.\"\n",
    "# BERTScore\n",
    "P, R, F1 = bert_score(hypotheses, references, lang=\"en\")  \n",
    "bertscore_f1 = F1.mean().item()\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "\n",
    "# BLEU Score\n",
    "# Ensure hypotheses and references are tokenized as lists of tokens\n",
    "hypotheses = [hyp.split() for hyp in df['deepl'].tolist()]  # Tokenized system translations\n",
    "references = [[ref.split()] for ref in df['text_en'].tolist()]  # Tokenized references (wrapped in a list for each)\n",
    "\n",
    "# Calculate BLEU using NLTK with smoothing function\n",
    "smoothing_function = SmoothingFunction().method4  # Use method4 for better handling of short sentences\n",
    "bleu_score = corpus_bleu(references, hypotheses, smoothing_function=smoothing_function)\n",
    "\n",
    "print(f\"BLEU Score: {bleu_score}\")\n",
    "\n",
    "# Append BLEU to the existing metrics\n",
    "metrics = {\n",
    "    \"SacreBLEU\": sacrebleu_score,\n",
    "    \"ROUGE-L\": average_rouge_l,\n",
    "    \"METEOR\": meteor,\n",
    "    \"TER\": ter,\n",
    "    \"ChrF\": chrf,\n",
    "    \"BERTScore F1\": bertscore_f1,\n",
    "    \"BLEU\": bleu_score\n",
    "}\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu import corpus_bleu\n",
    "\n",
    "hypotheses = df['translation'].tolist()  # List of system translations\n",
    "references = [[ref] for ref in df['text_en'].tolist()]  # Wrap each reference in a list for sacrebleu\n",
    "\n",
    "# Calculate SacreBLEU\n",
    "sacrebleu_score = corpus_bleu(hypotheses, references).score\n",
    "print(f\"SacreBLEU Score: {sacrebleu_score}\")\n",
    "\n",
    "# Calculate ROUGE-L\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "rouge_l_scores = [scorer.score(ref, hyp)['rougeL'].fmeasure for ref, hyp in zip(df['text_en'], df['translation'])]\n",
    "average_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores) if rouge_l_scores else 0.0\n",
    "print(f\"Average ROUGE-L Score: {average_rouge_l}\")\n",
    "\n",
    "# METEOR Score\n",
    "# Tokenize references and hypotheses\n",
    "references = [ref.split() for ref in df['text_en'].tolist()]  # Tokenize each reference translation\n",
    "hypotheses = [hyp.split() for hyp in df['translation'].tolist()]  # Tokenize each system translation\n",
    "\n",
    "# Calculate METEOR scores\n",
    "meteor_scores = [meteor_score([ref], hyp) for ref, hyp in zip(references, hypotheses)]\n",
    "meteor = sum(meteor_scores) / len(meteor_scores)\n",
    "\n",
    "# TER Score (using edit distance normalized by reference length)\n",
    "def ter_score(hypotheses: List[str], references: List[List[str]]) -> float:\n",
    "    total_edits = 0\n",
    "    total_ref_length = 0\n",
    "    for hyp, refs in zip(hypotheses, references):\n",
    "        ref = refs[0]\n",
    "        total_edits += nltk.edit_distance(hyp, ref)\n",
    "        total_ref_length += len(ref.split())\n",
    "    return total_edits / total_ref_length if total_ref_length > 0 else 0\n",
    "\n",
    "ter = ter_score(hypotheses, references)\n",
    "\n",
    "# ChrF score\n",
    "hypotheses = df['translation'].tolist()  # Machine-generated translations as a list of full sentences\n",
    "references = [df['text_en'].tolist()]  # Wrap in a list to match sacrebleu's expected format for multiple references\n",
    "\n",
    "# Calculate chrF score\n",
    "chrf = sacrebleu.corpus_chrf(hypotheses, references).score\n",
    "\n",
    "hypotheses = df['translation'].tolist()  # Machine-generated translations\n",
    "references = df['text_en'].tolist()  # Reference translations\n",
    "\n",
    "# Ensure lengths match\n",
    "assert len(hypotheses) == len(references), \"Mismatch in length between hypotheses and references.\"\n",
    "# BERTScore\n",
    "P, R, F1 = bert_score(hypotheses, references, lang=\"en\")  \n",
    "bertscore_f1 = F1.mean().item()\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "# BLEU Score\n",
    "# Ensure hypotheses and references are tokenized as lists of tokens\n",
    "hypotheses = [hyp.split() for hyp in df['translation'].tolist()]  # Tokenized system translations\n",
    "references = [[ref.split()] for ref in df['text_en'].tolist()]  # Tokenized references (wrapped in a list for each)\n",
    "\n",
    "# Calculate BLEU using NLTK with smoothing function\n",
    "smoothing_function = SmoothingFunction().method4  # Use method4 for better handling of short sentences\n",
    "bleu_score = corpus_bleu(references, hypotheses, smoothing_function=smoothing_function)\n",
    "\n",
    "print(f\"BLEU Score: {bleu_score}\")\n",
    "\n",
    "# Append BLEU to the existing metrics\n",
    "metrics = {\n",
    "    \"SacreBLEU\": sacrebleu_score,\n",
    "    \"ROUGE-L\": average_rouge_l,\n",
    "    \"METEOR\": meteor,\n",
    "    \"TER\": ter,\n",
    "    \"ChrF\": chrf,\n",
    "    \"BERTScore F1\": bertscore_f1,\n",
    "    \"BLEU\": bleu_score\n",
    "}\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu import corpus_bleu\n",
    "\n",
    "hypotheses = df['translated'].tolist()  # List of system translations\n",
    "references = [[ref] for ref in df['text_en'].tolist()]  # Wrap each reference in a list for sacrebleu\n",
    "\n",
    "# Calculate SacreBLEU\n",
    "sacrebleu_score = corpus_bleu(hypotheses, references).score\n",
    "print(f\"SacreBLEU Score: {sacrebleu_score}\")\n",
    "\n",
    "# Calculate ROUGE-L\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "rouge_l_scores = [scorer.score(ref, hyp)['rougeL'].fmeasure for ref, hyp in zip(df['text_en'], df['translated'])]\n",
    "average_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores) if rouge_l_scores else 0.0\n",
    "print(f\"Average ROUGE-L Score: {average_rouge_l}\")\n",
    "\n",
    "# METEOR Score\n",
    "# Tokenize references and hypotheses\n",
    "references = [ref.split() for ref in df['text_en'].tolist()]  # Tokenize each reference translation\n",
    "hypotheses = [hyp.split() for hyp in df['translated'].tolist()]  # Tokenize each system translation\n",
    "\n",
    "# Calculate METEOR scores\n",
    "meteor_scores = [meteor_score([ref], hyp) for ref, hyp in zip(references, hypotheses)]\n",
    "meteor = sum(meteor_scores) / len(meteor_scores)\n",
    "\n",
    "# TER Score (using edit distance normalized by reference length)\n",
    "def ter_score(hypotheses: List[str], references: List[List[str]]) -> float:\n",
    "    total_edits = 0\n",
    "    total_ref_length = 0\n",
    "    for hyp, refs in zip(hypotheses, references):\n",
    "        ref = refs[0]\n",
    "        total_edits += nltk.edit_distance(hyp, ref)\n",
    "        total_ref_length += len(ref.split())\n",
    "    return total_edits / total_ref_length if total_ref_length > 0 else 0\n",
    "\n",
    "ter = ter_score(hypotheses, references)\n",
    "\n",
    "# ChrF score\n",
    "hypotheses = df['translated'].tolist()  # Machine-generated translations as a list of full sentences\n",
    "references = [df['text_en'].tolist()]  # Wrap in a list to match sacrebleu's expected format for multiple references\n",
    "\n",
    "# Calculate chrF score\n",
    "chrf = sacrebleu.corpus_chrf(hypotheses, references).score\n",
    "\n",
    "hypotheses = df['translated'].tolist()  # Machine-generated translations\n",
    "references = df['text_en'].tolist()  # Reference translations\n",
    "\n",
    "# Ensure lengths match\n",
    "assert len(hypotheses) == len(references), \"Mismatch in length between hypotheses and references.\"\n",
    "# BERTScore\n",
    "P, R, F1 = bert_score(hypotheses, references, lang=\"en\")  \n",
    "bertscore_f1 = F1.mean().item()\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "# BLEU Score\n",
    "# Ensure hypotheses and references are tokenized as lists of tokens\n",
    "hypotheses = [hyp.split() for hyp in df['translated'].tolist()]  # Tokenized system translations\n",
    "references = [[ref.split()] for ref in df['text_en'].tolist()]  # Tokenized references (wrapped in a list for each)\n",
    "\n",
    "# Calculate BLEU using NLTK with smoothing function\n",
    "smoothing_function = SmoothingFunction().method4  # Use method4 for better handling of short sentences\n",
    "bleu_score = corpus_bleu(references, hypotheses, smoothing_function=smoothing_function)\n",
    "\n",
    "print(f\"BLEU Score: {bleu_score}\")\n",
    "\n",
    "# Append BLEU to the existing metrics\n",
    "metrics = {\n",
    "    \"SacreBLEU\": sacrebleu_score,\n",
    "    \"ROUGE-L\": average_rouge_l,\n",
    "    \"METEOR\": meteor,\n",
    "    \"TER\": ter,\n",
    "    \"ChrF\": chrf,\n",
    "    \"BERTScore F1\": bertscore_f1,\n",
    "    \"BLEU\": bleu_score\n",
    "}\n",
    "\n",
    "metrics\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import nltk\n",
    "import numpy as np\n",
    "import sacrebleu\n",
    "from sacrebleu import corpus_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from bert_score import score as bert_score\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "import torch\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Load data into a DataFrame\n",
    "df = pd.read_csv(\"data/output/qwen_7b.csv\")\n",
    "\n",
    "# Filter rows to only include those where 'text_translated' and 'text_en' are strings\n",
    "df = df[df['text_translated'].apply(lambda x: isinstance(x, str)) & df['text_en'].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "# Prepare hypotheses and references\n",
    "hypotheses = df['text_translated'].tolist()  # List of system translations\n",
    "references = [[ref] for ref in df['text_en'].tolist()]  # Wrap each reference in a list for sacrebleu\n",
    "\n",
    "# Calculate SacreBLEU\n",
    "sacrebleu_score = corpus_bleu(hypotheses, references).score\n",
    "print(f\"SacreBLEU Score: {sacrebleu_score}\")\n",
    "\n",
    "# Calculate ROUGE-L\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "rouge_l_scores = [scorer.score(ref, hyp)['rougeL'].fmeasure for ref, hyp in zip(df['text_en'], df['text_translated'])]\n",
    "average_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores) if rouge_l_scores else 0.0\n",
    "print(f\"Average ROUGE-L Score: {average_rouge_l}\")\n",
    "\n",
    "# METEOR Score\n",
    "# Tokenize references and hypotheses\n",
    "references = [ref.split() for ref in df['text_en'].tolist()]  # Tokenize each reference translation\n",
    "hypotheses = [hyp.split() for hyp in df['text_translated'].tolist()]  # Tokenize each system translation\n",
    "\n",
    "# Calculate METEOR scores\n",
    "meteor_scores = [meteor_score([ref], hyp) for ref, hyp in zip(references, hypotheses)]\n",
    "meteor = sum(meteor_scores) / len(meteor_scores)\n",
    "\n",
    "# TER Score (using edit distance normalized by reference length)\n",
    "def ter_score(hypotheses: List[str], references: List[List[str]]) -> float:\n",
    "    total_edits = 0\n",
    "    total_ref_length = 0\n",
    "    for hyp, refs in zip(hypotheses, references):\n",
    "        ref = refs[0]\n",
    "        total_edits += nltk.edit_distance(hyp, ref)\n",
    "        total_ref_length += len(ref.split())\n",
    "    return total_edits / total_ref_length if total_ref_length > 0 else 0\n",
    "\n",
    "ter = ter_score(df['text_translated'].tolist(), [[ref] for ref in df['text_en'].tolist()])\n",
    "\n",
    "# ChrF score\n",
    "hypotheses = df['text_translated'].tolist()  # Machine-generated translations as a list of full sentences\n",
    "references = [df['text_en'].tolist()]  # Wrap in a list to match sacrebleu's expected format for multiple references\n",
    "\n",
    "# Calculate chrF score\n",
    "chrf = sacrebleu.corpus_chrf(hypotheses, references).score\n",
    "\n",
    "# BERTScore\n",
    "P, R, F1 = bert_score(hypotheses, df['text_en'].tolist(), lang=\"en\")  \n",
    "bertscore_f1 = F1.mean().item()\n",
    "\n",
    "# Organize results\n",
    "metrics = {\n",
    "    \"BLEU\": sacrebleu_score,\n",
    "    \"ROUGE-L\": average_rouge_l,\n",
    "    \"METEOR\": meteor,\n",
    "    \"TER\": ter,\n",
    "    \"ChrF\": chrf,\n",
    "    \"BERTScore F1\": bertscore_f1\n",
    "}\n",
    "\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert equal length\n",
    "qwen_7 = pd.read_csv(\"data/annotation/qwen_7b_final.csv\")\n",
    "qwen_72 = pd.read_csv(\"data/annotation/qwen_72b_final.csv\")\n",
    "\n",
    "print(len(qwen_7), len(qwen_72))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_72.drop_duplicates(subset=['text_ja', 'image_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = pd.read_csv(\"data/annotation/annotation_cleaned.csv\")\n",
    "orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/output/qwen_72b_raw.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tag_text(text, tag, random=False): \n",
    "    '''\n",
    "    Extract text between two tags\n",
    "    '''\n",
    "    if random: \n",
    "        pattern = re.compile(rf'<{tag}>(.*?)<(.*?)>', re.DOTALL)\n",
    "    else:\n",
    "        pattern = re.compile(rf'<{tag}>(.*?)</{tag}>', re.DOTALL)\n",
    "    \n",
    "    matches = pattern.findall(text)\n",
    "    \n",
    "    return matches[0]\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "orig_df = pd.read_csv('data/annotation/annotation_cleaned.csv')\n",
    "new_df = pd.read_csv('data/output/qwen_7b_raw.csv')\n",
    "outputs_cleaned, orig_cleaned = [], []\n",
    "paths = sorted(list(set(orig_df.image_path.tolist())))\n",
    "for i, image_path in enumerate(tqdm(paths)): \n",
    "    orig = orig_df[orig_df['image_path'] == image_path].reset_index(drop=True)\n",
    "    new = new_df[new_df['image_path'] == image_path].reset_index(drop=True)\n",
    "    assert len(new) == 1\n",
    "    gtruth_len = len(orig)\n",
    "\n",
    "    for j in range(0, gtruth_len): \n",
    "        try: \n",
    "            content = extract_tag_text(new['outputs'].tolist()[0], f\"item_{j}\").strip()\n",
    "            o = extract_tag_text(content, f\"text\").strip()\n",
    "            t = extract_tag_text(content, f\"translation\").strip()\n",
    "            assert o == orig['text_ja'].tolist()[j].strip()\n",
    "            orig_cleaned.append(o)\n",
    "            outputs_cleaned.append(t)\n",
    "        except Exception as e: \n",
    "            print(gtruth_len)\n",
    "            print(f\"Missing content at index {j} of {image_path}\")\n",
    "            print(new['outputs'].tolist()[0])\n",
    "            print(orig['text_ja'].tolist())\n",
    "            orig_cleaned.append(orig['text_ja'].tolist()[j])\n",
    "            outputs_cleaned.append(\"N/A\")\n",
    "            break\n",
    "    \n",
    "\n",
    "orig_df['orig'] = orig_compared\n",
    "orig_df['translation'] = outputs_cleaned\n",
    "assert orig_df['text_ja'] == orig_df['orig'] \n",
    "orig_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = \"\"\"<item_0>    <text>     夢の翼は    </text>    <translation>     The wings of dreams are    </translation></item_0><item_1>    <text>     蝋で固めてある    </text>    <translation>     Fixed with a glue    </translation></item_1><item_2>    <text>     高く翔ぶほど    </text>    <translation>     The higher it flies    </translation></item_2><item_3>    <text>     太陽に溶かされてしまう    </text>    <translation>     It will be melted by the sun    </translation></item_3><item_4>    <text>     ーだったら    </text>    <translation>     If it were    </translation></item_4><item_5>    <text>     最初から翔ばない方がいい    </text>    <translation>     It's better not to fly from the start    </translation></item_5>\"\"\"\n",
    "def extract_tag_text(text, tag, random=False):\n",
    "    '''\n",
    "    Extract text between tags\n",
    "    '''\n",
    "    if random:\n",
    "        pattern = re.compile(rf'<{tag}>(.*?)<(.*?)>', re.DOTALL)\n",
    "    else:\n",
    "        pattern = re.compile(rf'<{tag}>(.*?)</{tag}>', re.DOTALL)\n",
    "    \n",
    "    matches = pattern.findall(text)\n",
    "    \n",
    "    return matches[0] if matches else None\n",
    "\n",
    "print(extract_tag_text(text, 'item_0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "\n",
    "df = pd.read_csv('data/output/qwen_7b.csv')\n",
    "# Extract translations for each row\n",
    "def extract_translations(row):\n",
    "    # Find all translations between <translation> tags\n",
    "    translations = re.findall(r\"<translation>(.*?)</translation>\", row['outputs'])\n",
    "    \n",
    "    # Match translation to text_ja in each row\n",
    "    if row['text_ja'] in row['outputs']:\n",
    "        # Find the index of matching Japanese text within <text> tags\n",
    "        text_matches = re.findall(r\"<text>(.*?)</text>\", row['outputs'])\n",
    "        try:\n",
    "            idx = text_matches.index(row['text_ja'])\n",
    "            # Return corresponding translation if available\n",
    "            return translations[idx] if idx < len(translations) else \"\"\n",
    "        except ValueError:\n",
    "            return \"\"\n",
    "    else:\n",
    "        print(\"Not detected\")\n",
    "        return \"\"\n",
    "\n",
    "# Apply function to each row\n",
    "df['extracted_translation'] = df.apply(extract_translations, axis=1)\n",
    "df.to_csv(\"data/output/qwen_7b.csv\", index=False)\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"data/output/qwen_7b.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('data/annotation/annotation_cleaned.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import nltk\n",
    "import numpy as np\n",
    "import sacrebleu\n",
    "from sacrebleu import corpus_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.translate.bleu_score import corpus_bleu as nltk_corpus_bleu\n",
    "from bert_score import score as bert_score\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "import torch\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Load data into a DataFrame\n",
    "df = pd.read_csv(\"data/output/qwen_7b.csv\")\n",
    "\n",
    "# Filter rows to only include those where 'text_translated' and 'text_en' are strings\n",
    "df = df[df['text_translated'].apply(lambda x: isinstance(x, str)) & df['text_en'].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "# Prepare hypotheses and references\n",
    "hypotheses = df['text_translated'].tolist()  # List of system translations\n",
    "references = [[ref.split()] for ref in df['text_en'].tolist()]  # Wrap each reference in a list for corpus_bleu\n",
    "\n",
    "# Calculate SacreBLEU\n",
    "sacrebleu_score = corpus_bleu(hypotheses, [[ref] for ref in df['text_en'].tolist()]).score\n",
    "print(f\"SacreBLEU Score: {sacrebleu_score}\")\n",
    "\n",
    "# Calculate BLEU using NLTK\n",
    "bleu_score = nltk_corpus_bleu(references, [hyp.split() for hyp in hypotheses])\n",
    "print(f\"BLEU Score (NLTK): {bleu_score}\")\n",
    "\n",
    "# Calculate ROUGE-L\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "rouge_l_scores = [scorer.score(ref, hyp)['rougeL'].fmeasure for ref, hyp in zip(df['text_en'], df['text_translated'])]\n",
    "average_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores) if rouge_l_scores else 0.0\n",
    "print(f\"Average ROUGE-L Score: {average_rouge_l}\")\n",
    "\n",
    "# METEOR Score\n",
    "# Tokenize references and hypotheses\n",
    "references = [ref.split() for ref in df['text_en'].tolist()]  # Tokenize each reference translation\n",
    "hypotheses = [hyp.split() for hyp in df['text_translated'].tolist()]  # Tokenize each system translation\n",
    "\n",
    "# Calculate METEOR scores\n",
    "meteor_scores = [meteor_score([ref], hyp) for ref, hyp in zip(references, hypotheses)]\n",
    "meteor = sum(meteor_scores) / len(meteor_scores)\n",
    "\n",
    "# TER Score (using edit distance normalized by reference length)\n",
    "def ter_score(hypotheses: List[str], references: List[List[str]]) -> float:\n",
    "    total_edits = 0\n",
    "    total_ref_length = 0\n",
    "    for hyp, refs in zip(hypotheses, references):\n",
    "        ref = refs[0]\n",
    "        total_edits += nltk.edit_distance(hyp, ref)\n",
    "        total_ref_length += len(ref.split())\n",
    "    return total_edits / total_ref_length if total_ref_length > 0 else 0\n",
    "\n",
    "ter = ter_score(df['text_translated'].tolist(), [[ref] for ref in df['text_en'].tolist()])\n",
    "\n",
    "# ChrF score\n",
    "hypotheses = df['text_translated'].tolist()  # Machine-generated translations as a list of full sentences\n",
    "references = [df['text_en'].tolist()]  # Wrap in a list to match sacrebleu's expected format for multiple references\n",
    "\n",
    "# Calculate chrF score\n",
    "chrf = sacrebleu.corpus_chrf(hypotheses, references).score\n",
    "\n",
    "# BERTScore\n",
    "P, R, F1 = bert_score(hypotheses, df['text_en'].tolist(), lang=\"en\")  \n",
    "bertscore_f1 = F1.mean().item()\n",
    "\n",
    "# Organize results\n",
    "metrics = {\n",
    "    \"SacreBLEU\": sacrebleu_score,\n",
    "    \"BLEU (NLTK)\": bleu_score,\n",
    "    \"ROUGE-L\": average_rouge_l,\n",
    "    \"METEOR\": meteor,\n",
    "    \"TER\": ter,\n",
    "    \"ChrF\": chrf,\n",
    "    \"BERTScore F1\": bertscore_f1\n",
    "}\n",
    "\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manga",
   "language": "python",
   "name": "manga"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
